rownames(train_data_t) <- colnames(train_data)[-1]  # Use the sample names as row names
# Add the Label column (Ensure the number of rows matches between train_data_t and sample_labels)
train_data_finals <- cbind(train_data_t, Label = sample_labels$Label)  # Add Label column
train_data_finals$Label <- as.factor(train_data_final$Label)
# Repeat for testing datalibrary(readxl)
# Identify sample names (excluding 'Gene_symbol')
sample_status <- colnames(test_data)[-1]
# Identify whether samples are lymph node negative or positive
status_binary <- ifelse(grepl("-IIIC$", sample_status), 0, 1)
# Convert to a dataframe & transpose (Make sure to keep the labels in the correct format)
sample_labels <- data.frame(Label = status_binary)
# Transpose test_data so that samples are rows and genes are columns
test_data_t <- as.data.frame(t(test_data[-1]))  # Exclude the 'Gene_symbol' column before transposing
colnames(test_data_t) <- test_data$Gene_symbol  # Rename columns using Gene_symbol
# Reset row names to avoid "V1, V2, ..." issue (using the sample names as row names)
rownames(test_data_t) <- colnames(test_data)[-1]  # Use the sample names as row names
# Add the Label column (Ensure the number of rows matches between test_data_t and sample_labels)
test_data_finals <- cbind(test_data_t, Label = sample_labels$Label)  # Add Label column
test_data_finals$Label <- as.factor(test_data_final$Label)
write_xlsx(train_data_finals, "training_dataset_final.xlsx")
write_xlsx(test_data_finals, "testing_dataset_final.xlsx")
library(readxl)
library(writexl)
# Load datasets
train_data_final <- read_excel("training_dataset_final.xlsx")
test_data_final <- read_excel("testing_dataset_final.xlsx")
train_data_final <- as.data.frame(train_data_final)
test_data_final <- as.data.frame(test_data_final)
# Logistic regression
#can tune hyperparameters
library(tidymodels)
library(glmnet)
logistic_regression_train <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification") %>%
fit(Label ~ ., data = train_data_final)
class(train_data_final)
rlang::last_trace()
rlang::last_trace(drop = FALSE)
library(readxl)
library(writexl)
# Load datasets
train_data_final <- read_excel("training_dataset_final.xlsx")
test_data_final <- read_excel("testing_dataset_final.xlsx")
train_data_final <- as.data.frame(train_data_final)
test_data_final <- as.data.frame(test_data_final)
# Logistic regression
#can tune hyperparameters
library(tidymodels)
library(glmnet)
logistic_regression_train <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification") %>%
fit(Label ~ ., data = train_data_final)
library(readxl)
library(writexl)
# Load datasets
train_data_final <- read_excel("training_dataset_final.xlsx")
test_data_final <- read_excel("testing_dataset_final.xlsx")
# Make sure both datasets are dataframe
train_data_final <- as.data.frame(train_data_final)
test_data_final <- as.data.frame(test_data_final)
# Make sure Label is a factor
train_data_final$Label <- as.factor(train_data_final$Label)
test_data_final$Label <- as.factor(test_data_final$Label)
# Logistic regression
#can tune hyperparameters
library(tidymodels)
library(glmnet)
logistic_regression_train <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification") %>%
fit(Label ~ ., data = train_data_final)
# Print model summary
summary(logistic_regression_train$fit)
dim(train_data_final)
dim(test_data_final)
#package, library
lr_prediction_test <- predict(logistic_regression_train, new_data = test_data_final)
# Compare predicted results to real data
summary(lr_prediction_test)
table(test_data_final$Label)
#Confusion matrix
library(caret)
confusionMatrix(data=lr_prediction_test$.pred_class, reference = test_data_final$Label)
# Remove all "-" characters and replace all "*" characters, so that RF can work
clean_names <- function(x) {
x <- gsub("[^A-Za-z0-9*]", "", x)  # Remove all special characters except *
x <- gsub("\\*", "2", x)           # Replace * with 2
return(x)
}
# Columns & rows replacement
colnames(train_data_final) <- clean_names(colnames(train_data_final))
rownames(train_data_final) <- clean_names(rownames(train_data_final))
# Repeat for testing
colnames(test_data_final) <- clean_names(colnames(test_data_final))
rownames(test_data_final) <- clean_names(rownames(test_data_final))
#remove all - & * (dupes) from the genes, samples etc (special characters)
#Random forest
library(randomForest)
set.seed(123)
random_forest_train <- randomForest(Label ~ .,
data = train_data_final)
summary(random_forest_train)
rf_prediction_test <- predict(random_forest_train, newdata = test_data_final)
summary(rf_prediction_test)
table(test_data_final$Label)
#Confusion matrix
confusionMatrix(data=rf_prediction_test, reference = test_data_final$Label)
#XGBoost
library(xgboost)
# Prepare for XGBoost
# Extract labels
train_labels <- as.numeric(as.factor(train_data_final$Label)) - 1
test_labels  <- as.numeric(as.factor(test_data_final$Label)) - 1
# Extract features (remove the label column)
train_matrix <- as.matrix(train_data_final[, setdiff(names(train_data_final), "Label")])
test_matrix  <- as.matrix(test_data_final[, setdiff(names(test_data_final), "Label")])
head(train_matrix)
class(train_matrix)
#class(train_matrix$kshviRK1210a)
#class(train_matrix$Label)
# Create DMatrix objects
dtrain <- xgb.DMatrix(data = train_matrix, label = train_labels)
dtest  <- xgb.DMatrix(data = test_matrix, label = test_labels)
getinfo(dtrain, "label")  # returns the labels
dim(dtrain)               # returns dimensions
# Run XGBoost
xgb_model <- xgboost(
data = dtrain,
nrounds = 100,
objective = "binary:logistic",
eval_metric = "logloss"
)
# Predict probabilities on the test set
xgb_prediction_test <- predict(xgb_model, dtest)
prediction <- as.numeric(xgb_prediction_test > 0.5)
print(head(prediction))
summary(prediction)
table(test_data_final$Label)
#Confusion matrix
# Labels as 0/1
labels <- as.numeric(as.factor(test_data_final$Label)) - 1
# Confusion matrix
confusionMatrix(
data = factor(prediction, levels = c(0, 1)),
reference = factor(labels, levels = c(0, 1))
)
library(readxl)
library(writexl)
# Load datasets
train_data_final <- read_excel("training_dataset_final.xlsx")
test_data_final <- read_excel("testing_dataset_final.xlsx")
# Make sure both datasets are dataframe
train_data_final <- as.data.frame(train_data_final)
test_data_final <- as.data.frame(test_data_final)
# Make sure Label is a factor
train_data_final$Label <- as.factor(train_data_final$Label)
test_data_final$Label <- as.factor(test_data_final$Label)
# Logistic regression
#can tune hyperparameters
library(tidymodels)
library(glmnet)
logistic_regression_train <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification") %>%
fit(Label ~ ., data = train_data_final)
# Print model summary
summary(logistic_regression_train$fit)
dim(train_data_final)
dim(test_data_final)
#package, library
lr_prediction_test <- predict(logistic_regression_train, new_data = test_data_final)
# Compare predicted results to real data
summary(lr_prediction_test)
table(test_data_final$Label)
#Confusion matrix
library(caret)
confusionMatrix(data=lr_prediction_test$.pred_class, reference = test_data_final$Label)
# Remove all "-" characters and replace all "*" characters, so that RF can work
clean_names <- function(x) {
x <- gsub("[^A-Za-z0-9*]", "", x)  # Remove all special characters except *
x <- gsub("\\*", "2", x)           # Replace * with 2
return(x)
}
# Columns & rows replacement
colnames(train_data_final) <- clean_names(colnames(train_data_final))
rownames(train_data_final) <- clean_names(rownames(train_data_final))
# Repeat for testing
colnames(test_data_final) <- clean_names(colnames(test_data_final))
rownames(test_data_final) <- clean_names(rownames(test_data_final))
#remove all - & * (dupes) from the genes, samples etc (special characters)
#Random forest
library(randomForest)
set.seed(123)
random_forest_train <- randomForest(Label ~ .,
data = train_data_final)
summary(random_forest_train)
rf_prediction_test <- predict(random_forest_train, newdata = test_data_final)
summary(rf_prediction_test)
table(test_data_final$Label)
#Confusion matrix
confusionMatrix(data=rf_prediction_test, reference = test_data_final$Label)
#XGBoost
library(xgboost)
# Prepare for XGBoost
# Extract labels
train_labels <- as.numeric(as.factor(train_data_final$Label)) - 1
test_labels  <- as.numeric(as.factor(test_data_final$Label)) - 1
# Extract features (remove the label column)
train_matrix <- as.matrix(train_data_final[, setdiff(names(train_data_final), "Label")])
test_matrix  <- as.matrix(test_data_final[, setdiff(names(test_data_final), "Label")])
#head(train_matrix)
#class(train_matrix)
#class(train_matrix$kshviRK1210a)
#class(train_matrix$Label)
# Create DMatrix objects
dtrain <- xgb.DMatrix(data = train_matrix, label = train_labels)
dtest  <- xgb.DMatrix(data = test_matrix, label = test_labels)
getinfo(dtrain, "label")  # returns the labels
dim(dtrain)               # returns dimensions
# Run XGBoost
xgb_model <- xgboost(
data = dtrain,
nrounds = 100,
objective = "binary:logistic",
eval_metric = "logloss"
)
# Predict probabilities on the test set
xgb_prediction_test <- predict(xgb_model, dtest)
prediction <- as.numeric(xgb_prediction_test > 0.5)
print(head(prediction))
summary(prediction)
table(test_data_final$Label)
#Confusion matrix
# Labels as 0/1
labels <- as.numeric(as.factor(test_data_final$Label)) - 1
# Confusion matrix
confusionMatrix(
data = factor(prediction, levels = c(0, 1)),
reference = factor(labels, levels = c(0, 1))
)
# Import necessary libraries
library(readxl)
library(writexl)
library(tidymodels)
library(glmnet)
library(caret)
# Load datasets
train_data_final <- read_excel("training_dataset_final.xlsx")
test_data_final <- read_excel("testing_dataset_final.xlsx")
# Make sure both datasets are dataframe
train_data_final <- as.data.frame(train_data_final)
test_data_final <- as.data.frame(test_data_final)
# Make sure Label is a factor
train_data_final$Label <- as.factor(train_data_final$Label)
test_data_final$Label <- as.factor(test_data_final$Label)
# Test hyper parameters
# Logistic regession
# Define training control with random search
ctrl <- trainControl(method = "cv", number = 5, search = "random")
# Train logistic regression
set.seed(123)
model_glm <- train(Class ~ ., data = train_data_final,
method = "glm",
family = "binomial",
trControl = ctrl,
tuneLength = 10)  # number of random combos
# Import necessary libraries
library(readxl)
library(writexl)
library(tidymodels)
library(glmnet)
library(caret)
# Load datasets
train_data_final <- read_excel("training_dataset_final.xlsx")
test_data_final <- read_excel("testing_dataset_final.xlsx")
# Make sure both datasets are dataframe
train_data_final <- as.data.frame(train_data_final)
test_data_final <- as.data.frame(test_data_final)
# Make sure Label is a factor
train_data_final$Label <- as.factor(train_data_final$Label)
test_data_final$Label <- as.factor(test_data_final$Label)
# Test hyper parameters
# Logistic regession
# Define training control with random search
ctrl <- trainControl(method = "cv", number = 5, search = "random")
# Train logistic regression
set.seed(123)
model_glm <- train(Label ~ ., data = train_data_final,
method = "glm",
family = "binomial",
trControl = ctrl,
tuneLength = 10)  # number of random combos
summary(model_glm)
# Random forest
ctrl <- trainControl(method = "cv", number = 5, search = "random")
set.seed(123)
model_rf <- train(Label ~ ., data = train_data_final,
method = "rf",
trControl = ctrl,
tuneLength = 10)  # random 10 sets of mtry
print(model_rf)
# XGBoost
ctrl <- trainControl(method = "cv", number = 5, search = "random")
set.seed(123)
model_xgb <- train(Label ~ ., data = train_data_final,
method = "xgbTree",
trControl = ctrl,
tuneLength = 10)  # randomly sample 10 param combos
print(model_xgb)
# Import necessary libraries
library(readxl)
library(writexl)
library(tidymodels)
library(glmnet)
library(caret)
# Load datasets
train_data_final <- read_excel("training_dataset_final.xlsx")
test_data_final <- read_excel("testing_dataset_final.xlsx")
# Make sure both datasets are dataframe
train_data_final <- as.data.frame(train_data_final)
test_data_final <- as.data.frame(test_data_final)
# Make sure Label is a factor
train_data_final$Label <- as.factor(train_data_final$Label)
test_data_final$Label <- as.factor(test_data_final$Label)
# Test hyper parameters
# Logistic regession
# Define training control with random search
ctrl <- trainControl(method = "cv", number = 5, search = "random")
# Number of variables randomly sampled as candidates at each split
tuneGrid <- expand.grid(.mtry = c(1:5))
# Train logistic regression
set.seed(123)
model_glm <- train(Label ~ ., data = train_data_final,
method = "glm",
family = "binomial",
tuneGrid = tuneGrid,
trControl = ctrl,
tuneLength = 10)  # number of random combos
# Import necessary libraries
library(readxl)
library(writexl)
library(tidymodels)
library(glmnet)
library(caret)
# Load datasets
train_data_final <- read_excel("training_dataset_final.xlsx")
test_data_final <- read_excel("testing_dataset_final.xlsx")
# Make sure both datasets are dataframe
train_data_final <- as.data.frame(train_data_final)
test_data_final <- as.data.frame(test_data_final)
# Make sure Label is a factor
train_data_final$Label <- as.factor(train_data_final$Label)
test_data_final$Label <- as.factor(test_data_final$Label)
# Test hyper parameters
# Logistic regession
# Define training control with random search
ctrl <- trainControl(method = "cv", number = 5, search = "random")
# Number of variables randomly sampled as candidates at each split
tuneGrid <- expand.grid(.mtry = c(1:5))
# Train logistic regression
set.seed(123)
model_glm <- train(Label ~ ., data = train_data_final,
method = "glm",
family = "binomial",
trControl = ctrl,
tuneLength = 10)  # number of random combos
summary(model_glm)
# Random forest
set.seed(123)
model_rf <- train(Label ~ ., data = train_data_final,
method = "rf",
tuneGrid = tuneGrid,
trControl = ctrl,
tuneLength = 10)  # random 10 sets of mtry
model_glm$bestTune
model_glm$results
# Import necessary libraries
library(readxl)
library(writexl)
library(tidymodels)
library(glmnet)
library(caret)
# Load datasets
train_data_final <- read_excel("training_dataset_final.xlsx")
test_data_final <- read_excel("testing_dataset_final.xlsx")
# Make sure both datasets are dataframe
train_data_final <- as.data.frame(train_data_final)
test_data_final <- as.data.frame(test_data_final)
# Make sure Label is a factor
train_data_final$Label <- as.factor(train_data_final$Label)
test_data_final$Label <- as.factor(test_data_final$Label)
# Test hyper parameters
# Logistic regession
# Define training control with random search
ctrl <- trainControl(method = "cv", number = 5, search = "random")
# Number of variables randomly sampled as candidates at each split
tuneGrid <- expand.grid(.mtry = c(1:5))
# Train logistic regression
set.seed(123)
model_glm <- train(Label ~ ., data = train_data_final,
method = "glm",
family = "binomial",
trControl = ctrl,
tuneLength = 10)  # number of random combos
summary(model_glm)
model_glmnet$results
# Import necessary libraries
library(readxl)
library(writexl)
library(tidymodels)
library(glmnet)
library(caret)
# Load datasets
train_data_final <- read_excel("training_dataset_final.xlsx")
test_data_final <- read_excel("testing_dataset_final.xlsx")
# Make sure both datasets are dataframe
train_data_final <- as.data.frame(train_data_final)
test_data_final <- as.data.frame(test_data_final)
# Make sure Label is a factor
train_data_final$Label <- as.factor(train_data_final$Label)
test_data_final$Label <- as.factor(test_data_final$Label)
# Test hyper parameters
# Logistic regession
# Define training control with random search
ctrl <- trainControl(method = "cv", number = 5, search = "random")
# Number of variables randomly sampled as candidates at each split
tuneGrid <- expand.grid(.mtry = c(1:5))
# Train logistic regression
set.seed(123)
model_glm <- train(Label ~ ., data = train_data_final,
method = "glm",
family = "binomial",
trControl = ctrl,
tuneLength = 10)  # number of random combos
summary(model_glm)
model_glm$results
model_glmt$bestTune
summary(model_glm)
model_glm$results
model_glm$bestTune
model_glm$finalModel
plot(model_glm)
predict(model_glm, newdata = test_data_final)
View(train_data_t)
library(readxl)
library(writexl)
# Load dataset
excel_data <- read_excel("cleaned_dataset.xlsx")
excel_data <- as.data.frame(excel_data)
# Store gene symbols before removing them
row.names(excel_data) <- excel_data[[1]]
excel_data <- excel_data[, -1]
head(excel_data)
# Function to split dataset into training (70%) and testing (30%)
split_dataset <- function(df, train_ratio = 0.7) {
set.seed(123)
total_cols <- ncol(df)
train_indices <- sample(total_cols, size = round(train_ratio * total_cols))
train_set <- df[, train_indices, drop = FALSE]
test_set <- df[, -train_indices, drop = FALSE]
return(list(train = train_set, test = test_set))
}
# Function for manual quantile normalization
quantile_normalization <- function(train_df, test_df) {
# Step 1: Sort each column and store original row indices
sorted_ord <- apply(train_df, 2, order)  # Get sorted indices
sorted_data <- apply(train_df, 2, sort)  # Sort values
# Step 2: Compute row-wise means
row_means <- rowMeans(sorted_data)
# Step 3: Assign normalized values back to original training ranks
normalized_train <- matrix(row_means[sorted_ord], ncol=ncol(train_df))
dimnames(normalized_train) <- dimnames(train_df)
#Check if step 3 worked
print(sorted_data[1:6,1:6])
print(sorted_ord[1:6,1:6])
print(normalized_train[1:6,1:6])
# Step 4: Normalize the test set using interpolation
sorted_ord_test <- apply(test_df, 2, order)  # Get sorted indices
sorted_data_test <- apply(test_df, 2, sort)#row means train data
normalized_test <- matrix(row_means[sorted_ord_test], ncol=ncol(test_df))
dimnames(normalized_test) <- dimnames(test_df)
#Check if step 4 worked
print(sorted_data_test[1:6,1:6])
print(sorted_ord_test[1:6,1:6])
print(normalized_test[1:6,1:6])
# Convert back to data frames and reattach gene symbols
train_genes <- row.names(train_df)
test_genes <- row.names(test_df)
normalized_train_df <- data.frame(Gene = train_genes, normalized_train)
normalized_test_df <- data.frame(Gene = test_genes, normalized_test)
# Set the correct column names for the normalized data frames
#"gene needed, otherwise gene symbol names infiltrate the actual dataset"
colnames(normalized_train_df) <- c("Gene_symbol", colnames(train_df))  # Ensure first column is "Gene"
colnames(normalized_test_df) <- c("Gene_symbol", colnames(test_df))    # Ensure first column is "Gene"
return(list(train = normalized_train_df, test = normalized_test_df))
}
# Split dataset
split_data <- split_dataset(excel_data)
# Apply manual quantile normalization
normalized_data <- quantile_normalization(split_data$train, split_data$test)
# Check output
head(normalized_data$train)
# Boxplot before and after normalization
#smaller font
par(mar = c(10, 4, 4, 2))
boxplot(as.matrix(excel_data), main = "Before Normalization", las=2)
boxplot(as.matrix(normalized_data$train[,-1]), main = "After Normalization", las=2)
# Export data
write_xlsx(normalized_data$train, "normalized_training_dataset.xlsx")
write_xlsx(normalized_data$test, "normalized_testing_dataset.xlsx")
